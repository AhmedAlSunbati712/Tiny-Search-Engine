# Indexer - Assumptions & Implementation

## Assumptions

- **Crawler Validation**: The program checks for the existence of a `.crawler` file in the provided page directory to verify that the directory was generated by the crawler.
- **Writable Output File**: It is assumed that the output file path provided is writable.
- **Minimum Word Length**: Only words with length **≥ 3 characters** are indexed.
- **Memory Allocation**: All memory allocations are checked with a custom `mem_assert`.
- **Webpage Fetching**: `webpage_fetch` is called on each loaded webpage to ensure HTML content is present.

## Implementation Spec
We will cover the following topics:
- Data Structures
- Control Flow
- Detailed function prototypes and their parameters
- Error handling and recovery
- Testing plan

## Data Structures
The indexer uses an `index` object of words mapped to a counter set that contains counters for each document page ID. The count in each counter refers to the number of occurences of that given word in the documentID in the same counter. It also uses a hashtable inside `indexPage` to keep track of what words have been scanned so far and how many times they have been encountered in the page. Furthermore, it includes a struct internally called `indexDocumentPair` that has two attributes: `index`, and `docID`. It's used to hold an index to insert a word and its counter (paired with the `docID` in the `indexDocumentPair` struct) to the index given in `indexDocumentPair`.

## Control Flow
The indexer is implemented in one file `indexer.c`, with 6 functions:
### main
The `main` function calls `parseArgs` -> `buildIndex` -> `indexSave` and checks whether its execution was successful. If successful -> `index_delete` and exits with 0. If `indexSave` doesn't execute successfully, it prints an error message and exits with 1.
### parseArgs
Given three arguments from the command line, it extracts them into the function parameters; return only if successful.
- Checks if the number of arguments `argc > 3`.
- Parses the second argument into `pageDirectory`.
- Parses the third argument into `indexFileName`.
### indexBuild
Builds an index given `pageDirectory`.
```
initialize index with typical size
docID ← 1

while file at "pageDirectory/docID" exists:
    fp ← open file at path
    read first line from file (URL of the page)
    read second line from file (Depth of the page)
    create new webpage with pageURL and depth
    fetch HTML content for page
    scan the page for words using indexPage
    delete page
    close file
    increment docID by one
return index
```
### indexPage
Scans a word in a page given a pointer to a `webpage_t` struct, a pointer to an index, and the `docID`.
```
create a new hashtable to track words and their counts
while there's a next word in the webpage:
    if word length > 3:
        normalize the word
        if word not in hashtable:
            allocate space for count and set count to 1
            insert word and count into hashtable
        else:
            increment existing count

create a new indexDoc struct with index and docID
for each word in hashtable:
    insert the word and its count into the index under docID

delete the hashtable and free all counts
free indexDoc
```
### formatPath
Helper function that given a `pageDirectory` and a `docID` for a doucment inside the `pageDirectory`, it allocates memory and returns the path `pageDirectory/docID`.
```
allocate memory for path string
format the string as "pageDirectory/docID"
return the formatted path
```
### insertWordIntoIndex
Given an `indexDocumentPair` struct, a word, and the `count` for the word in the `docID` we are currently scanning:
```
extract index and docID from indexAndDocument
insert the word into the index with docID and count
```

### normalizeWord
Takes an input string `word`, allocates memory for a new string and stores the lowercase version of `word` and return it:
```
if word is NULL:
    return NULL

allocate memory for normalizedWord with size equal to word length + 1
for each character in word:
    convert character to lowercase and store in normalizedWord
append null character at the end of normalizedWord
return normalizedWord
```

## Other Modules
### index
We create a reusable module `index.c` to handle operations such as loading index from a file and saving index to a file. It aliases the hasthable data strucutre to a datastructure called `index_t` with keys as words and values as counters sets. It also includes two extra useful functions `index_save`, which saves an index content into a file with `filename` passed to it as one of its parameters, `index_load` which does the opposite; loading data from an index file to an index.
### index_save
Takes a pointer to an index, and a string of the filename to be saved to. Returns true if successful and false otherwise:
```
if index is NULL or filename is NULL:
    return false

open file with filename for writing
if file failed to open:
    return false

for each word in the index:
    write the word and its counter set to the file

close the file
return true
```
### index_load
Takes a filename to load from and returns an `index` with the given data in the filename:
```
if filename is NULL:
    return NULL

open file for reading
if file failed to open:
    return NULL

count lines in file
initialize new index with that size

for each line in file:
    read the first word
    create new counter set
    for each (docID, count) pair in line:
        add to counter set
    insert word and counter set into index
    free line

close file
return index
```
## libcs50
We leverage the modules of libcs50, mainly making use of `hashtable`, `counters`, `file`, `mem`, and `webpage`.
# Function prototypes
## indexer
Detailed descriptions of each function is given in `indexer.c`:
```c
static void parseArgs(const int argc, const char* argv[], const char** pageDirectory, const char** indexFileName);
index_t* indexBuild(const char* pageDirectory);
void indexPage(webpage_t* webpage, index_t* index, int docID);
static char* formatPath(const char* pageDirectory, int docID);
static void insertWordIntoIndex(void* indexAndDocument, const char* word, void* count);
char* normalizeWord(const char* word);
```
## index
Detailed descriptions of each function is given in `index.c`:
```c
index_t* index_new(const int num_slots);
bool index_insert(index_t *index, const char *word, const int docID, const int count);
counters_t* index_find(index_t* index, const char* word);
bool index_save(index_t *index, const char *filename);
index_t* index_load(const char* filename);
void index_delete(index_t *index);
static void counters_delete_helper(void *item);
static void counters_save_item(void *fp, const int docID, const int count);
static void index_save_item(void *fp, const char *word, void *item);
```
# Error-handling & Recovery
Out-of-memory errors are handled by variants of the mem_assert functions, which result in a message printed to stderr and a non-zero exit status. We anticipate out-of-memory errors to be rare and thus allow the program to crash (cleanly) in this way.

All code uses defensive-programming tactics to catch and exit (using variants of the mem_assert functions), e.g., if a function receives bad parameters.

Errors that can arise from saving an index file are also checked. If an error is encountered while trying to save an index, the program exits with code 1 and prints and error message to `stderr`.

# Testing Plan
- Test indexer against multiple invalid arguments
- Test indexer with multiple pageDirectories
- Test indextest and indexer for memory leaks